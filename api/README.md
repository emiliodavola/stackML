# API para Servir Modelos

Esta carpeta contiene una API FastAPI para servir modelos de Machine Learning registrados en MLflow.

## üìÅ Contenido

- `Dockerfile` - Imagen Docker para la API
- `server.py` - Servidor FastAPI principal
- `requirements.txt` - Dependencias Python

## üöÄ Funcionalidades

### Endpoints Disponibles

- `GET /` - Health check de la API
- `GET /docs` - Documentaci√≥n autom√°tica de la API (Swagger UI)
- `POST /predict` - Endpoint para realizar predicciones (cuando se implemente)

### Caracter√≠sticas

- **FastAPI**: Framework moderno y r√°pido para APIs
- **Integraci√≥n MLflow**: Carga modelos desde el Model Registry
- **Documentaci√≥n autom√°tica**: Swagger UI incluido
- **Contenedorizado**: Listo para deployment con Docker

## üõ†Ô∏è Uso

### Levantar la API

```bash
# Desde el directorio ra√≠z del proyecto
docker compose --profile on-demand up api -d
```

### Acceder a la API

- **API Base**: http://localhost:8000
- **Documentaci√≥n**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000

### Desarrollo Local

```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar localmente
uvicorn server:app --reload --port 8000
```

## üîß Configuraci√≥n

### Variables de Entorno

- `MLFLOW_TRACKING_URI` - URL del servidor MLflow (configurado autom√°ticamente)

### Personalizaci√≥n

Para agregar nuevos endpoints o modificar el comportamiento:

1. Editar `server.py`
2. Actualizar `requirements.txt` si es necesario
3. Rebuild la imagen: `docker compose build api`

## üìä Integraci√≥n con MLflow

La API est√° configurada para:

- Conectarse autom√°ticamente al servidor MLflow
- Cargar modelos desde el Model Registry
- Servir predicciones usando modelos registrados

## üîÆ Pr√≥ximas Funcionalidades

- [ ] Endpoint de predicci√≥n completamente funcional
- [ ] Carga din√°mica de modelos
- [ ] Logging de requests y respuestas
- [ ] M√©tricas de performance
- [ ] Autenticaci√≥n y autorizaci√≥n
- [ ] Rate limiting
- [ ] Versionado de modelos autom√°tico
